{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection\n%glue_version 3.0\n%region us-west-2\n%worker_type G.1X\n%number_of_workers 3\n%spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer",
			"metadata": {
				"trusted": true
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "Connections to be included:\nhudi-connection\nSetting Glue version to: 3.0\nPrevious region: us-west-2\nSetting new region to: us-west-2\nReauthenticating Glue client with new region: us-west-2\nIAM role has been set to arn:aws:iam::043916019468:role/Lab3. Reauthenticating.\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nAuthentication done.\nRegion is set to: us-west-2\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 3\nSetting new number of workers to: 3\nPrevious Spark configuration: spark.serializer=org.apache.spark.serializer.KryoSerializer\nSetting new Spark configuration to: spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 361c95d5-d328-4718-bdcd-916567bcc661\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\nfrom pyspark.sql.functions import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql.types import *\nfrom datetime import datetime\nimport boto3\nfrom functools import reduce\n",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 3\nSession ID: e75e9145-bc11-4c08-b442-81ff2d602514\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer\nWaiting for session e75e9145-bc11-4c08-b442-81ff2d602514 to get into ready status...\nSession e75e9145-bc11-4c08-b442-81ff2d602514 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 1:  Create Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.config('spark.serializer','org.apache.spark.serializer.KryoSerializer').config('spark.sql.hive.convertMetastoreParquet','false').config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#  Dimension -> Customers",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import udf\nimport time\n\nrandom_udf = udf(lambda: str(int(time.time() * 1000000)), StringType()) \n\ndim_customer_schema = StructType([\n        StructField('customer_id', StringType(), False),\n        StructField('first_name', StringType(), True),\n        StructField('last_name', StringType(), True),\n        StructField('city', StringType(), True),\n        StructField('country', StringType(), True),\n        StructField('eff_start_date', DateType(), True),\n        StructField('eff_end_date', DateType(), True),\n        StructField('timestamp', TimestampType(), True),\n        StructField('is_current', BooleanType(), True),\n    ])\n\ncustomer_dim_df = spark.createDataFrame([('1', 'John', 'Smith', \n                    'London', 'UK', \n                    datetime.strptime('2020-09-27', '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'),\n                    True),\n                    ('2', 'Susan', 'Chas', \n                    'Seattle', 'US',\n                    datetime.strptime('2020-10-14', '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'),\n                    True)], dim_customer_schema)\n\ncustomer_hudi_df = customer_dim_df.withColumn(\"customer_dim_key\", random_udf())\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customer_hudi_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|          1|      John|    Smith| London|     UK|    2020-09-27|  2999-12-31|2020-12-08 09:15:32|      true|1671069844246049|\n|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2999-12-31|2020-12-08 09:15:32|      true|1671069844601802|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"customers_table\"\n\nrecordkey = 'customer_dim_key'\nprecombine = 'timestamp'\n\npath = 's3://glue-learn-begineers/hudi/customers_table/'",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(customer_hudi_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options={\n            \"path\": path,\n            \"connectionName\": \"hudi-connection\",\n\n            \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n            'className': 'org.apache.hudi',\n            'hoodie.table.name': table_name,\n            'hoodie.datasource.write.recordkey.field': recordkey,\n            'hoodie.datasource.write.table.name': table_name,\n            'hoodie.datasource.write.operation': 'upsert',\n            'hoodie.datasource.write.precombine.field': precombine,\n\n\n            'hoodie.datasource.hive_sync.enable': 'true',\n            \"hoodie.datasource.hive_sync.mode\":\"hms\",\n            'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n            'hoodie.datasource.hive_sync.database': db_name,\n            'hoodie.datasource.hive_sync.table': table_name,\n            'hoodie.datasource.hive_sync.use_jdbc': 'false',\n            'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n            'hoodie.datasource.write.hive_style_partitioning': 'true',\n        },\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Fact Table -> Sales",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "fact_sales_schema = StructType([\n        StructField('item_id', StringType(), True),\n        StructField('quantity', IntegerType(), True),\n        StructField('price', DoubleType(), True),\n        StructField('timestamp', TimestampType(), True),\n        StructField('customer_id', StringType(), True)\n    ])\n\nsales_fact_df = spark.createDataFrame([('100', 25, 123.46,\n                    datetime.strptime('2020-11-17 09:15:32', '%Y-%m-%d %H:%M:%S'), '1'),\n                                       ('101', 300, 123.46,\n                    datetime.strptime('2020-10-28 09:15:32', '%Y-%m-%d %H:%M:%S'), '1'),\n                                      ('102', 5, 1038.0,\n                    datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'), '2')], \n                    fact_sales_schema)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sales_fact_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+--------+------+-------------------+-----------+\n|item_id|quantity| price|          timestamp|customer_id|\n+-------+--------+------+-------------------+-----------+\n|    100|      25|123.46|2020-11-17 09:15:32|          1|\n|    101|     300|123.46|2020-10-28 09:15:32|          1|\n|    102|       5|1038.0|2020-12-08 09:15:32|          2|\n+-------+--------+------+-------------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"sales_table\"\n\nrecordkey = 'item_id'\n\nprecombine = 'timestamp'\n\npath = 's3://glue-learn-begineers/hudi/sales_table/'\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(sales_fact_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options={\n            \"path\": path,\n            \"connectionName\": \"hudi-connection\",\n\n            \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n            'className': 'org.apache.hudi',\n            'hoodie.table.name': table_name,\n            'hoodie.datasource.write.recordkey.field': recordkey,\n            'hoodie.datasource.write.table.name': table_name,\n            'hoodie.datasource.write.operation': 'upsert',\n            'hoodie.datasource.write.precombine.field': precombine,\n\n\n            'hoodie.datasource.hive_sync.enable': 'true',\n            \"hoodie.datasource.hive_sync.mode\":\"hms\",\n            'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n            'hoodie.datasource.hive_sync.database': db_name,\n            'hoodie.datasource.hive_sync.table': table_name,\n            'hoodie.datasource.hive_sync.use_jdbc': 'false',\n            'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n            'hoodie.datasource.write.hive_style_partitioning': 'true',\n        },\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Joining  Dimension and Facts",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import when\n\njoin_cond = [sales_fact_df.customer_id == customer_hudi_df.customer_id,\n             sales_fact_df.timestamp >= customer_hudi_df.eff_start_date,\n             sales_fact_df.timestamp < customer_hudi_df.eff_end_date]\n\ncustomers_dim_key_df = (sales_fact_df\n                          .join(customer_hudi_df, join_cond, 'leftouter')\n                          .select(sales_fact_df['*'],\n                            when(customer_hudi_df.customer_dim_key.isNull(), '-1')\n                                  .otherwise(customer_hudi_df.customer_dim_key)\n                                  .alias(\"customer_dim_key\") )\n                       )",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customers_dim_key_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+--------+------+-------------------+-----------+----------------+\n|item_id|quantity| price|          timestamp|customer_id|customer_dim_key|\n+-------+--------+------+-------------------+-----------+----------------+\n|    100|      25|123.46|2020-11-17 09:15:32|          1|1671070039945039|\n|    101|     300|123.46|2020-10-28 09:15:32|          1|1671070039945039|\n|    102|       5|1038.0|2020-12-08 09:15:32|          2|1671070039977207|\n+-------+--------+------+-------------------+-----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Changes Needs to be done on Warehouse",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "new_customer_dim_df = spark.createDataFrame([('3', 'Bastian', 'Back', 'Berlin', 'GE',\n                    datetime.strptime(datetime.today().strftime('%Y-%m-%d'), '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-09 09:15:32', '%Y-%m-%d %H:%M:%S'), True),\n                    ('2', 'Susan', 'Chas','Paris', 'FR',\n                    datetime.strptime(datetime.today().strftime('%Y-%m-%d'), '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-09 10:15:32', '%Y-%m-%d %H:%M:%S'), True)],\n                dim_customer_schema)\nnew_customer_dim_df = new_customer_dim_df.withColumn(\"customer_dim_key\", random_udf())\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "new_customer_dim_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+------+-------+--------------+------------+-------------------+----------+----------------+\n|customer_id|first_name|last_name|  city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n+-----------+----------+---------+------+-------+--------------+------------+-------------------+----------+----------------+\n|          3|   Bastian|     Back|Berlin|     GE|    2022-12-15|  2999-12-31|2020-12-09 09:15:32|      true|1671070118785997|\n|          2|     Susan|     Chas| Paris|     FR|    2022-12-15|  2999-12-31|2020-12-09 10:15:32|      true|1671070118910573|\n+-----------+----------+---------+------+-------+--------------+------------+-------------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### What is Slowly Changing Dimension ?\n* A Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.\n\nIn this Case Customer Susan moved to  Paris from Seattle\nwe can handel this folloowing ways \n\n### Type 1 SCDs - Overwriting\n\nIn a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\n\n### Type 2 SCDs - Creating another dimension record\n\nA Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\n\n### Type 3 SCDs - Creating a current value field\n\nA Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value.\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "join_cond = [customer_hudi_df.customer_id == new_customer_dim_df.customer_id,\n             customer_hudi_df.is_current == True]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## Find customer records to update\ncustomers_to_update_df = (customer_hudi_df\n                          .join(new_customer_dim_df, join_cond)\n                          .select(customer_hudi_df.customer_id,\n                                  customer_hudi_df.first_name,\n                                  customer_hudi_df.last_name,\n                                  customer_hudi_df.city,\n                                  customer_hudi_df.country,\n                                  customer_hudi_df.eff_start_date,\n                                  new_customer_dim_df.eff_start_date.alias('eff_end_date'),\n                                  customer_hudi_df.customer_dim_key,\n                                  customer_hudi_df.timestamp)\n                          .withColumn('is_current', lit(False))\n                         )",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customers_to_update_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+-------+-------+--------------+------------+----------------+-------------------+----------+\n|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|customer_dim_key|          timestamp|is_current|\n+-----------+----------+---------+-------+-------+--------------+------------+----------------+-------------------+----------+\n|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2022-12-15|1671070148208987|2020-12-08 09:15:32|     false|\n+-----------+----------+---------+-------+-------+--------------+------------+----------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## Union with new customer records\nmerged_customers_df = new_customer_dim_df. unionByName(customers_to_update_df)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "merged_customers_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|          3|   Bastian|     Back| Berlin|     GE|    2022-12-15|  2999-12-31|2020-12-09 09:15:32|      true|1671070198638529|\n|          2|     Susan|     Chas|  Paris|     FR|    2022-12-15|  2999-12-31|2020-12-09 10:15:32|      true|1671070198895047|\n|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2022-12-15|2020-12-08 09:15:32|     false|1671070198141873|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"customers_table\"\n\nrecordkey = 'customer_dim_key'\nprecombine = 'timestamp'\n\npath = 's3://glue-learn-begineers/hudi/customers_table/'",
			"metadata": {
				"trusted": true
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(merged_customers_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options={\n            \"path\": path,\n            \"connectionName\": \"hudi-connection\",\n\n            \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n            'className': 'org.apache.hudi',\n            'hoodie.table.name': table_name,\n            'hoodie.datasource.write.recordkey.field': recordkey,\n            'hoodie.datasource.write.table.name': table_name,\n            'hoodie.datasource.write.operation': 'upsert',\n            'hoodie.datasource.write.precombine.field': precombine,\n\n\n            'hoodie.datasource.hive_sync.enable': 'true',\n            \"hoodie.datasource.hive_sync.mode\":\"hms\",\n            'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n            'hoodie.datasource.hive_sync.database': db_name,\n            'hoodie.datasource.hive_sync.table': table_name,\n            'hoodie.datasource.hive_sync.use_jdbc': 'false',\n            'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n            'hoodie.datasource.write.hive_style_partitioning': 'true',\n        },\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"ok\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "ok\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Referneces \n* https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/",
			"metadata": {}
		}
	]
}